{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde21b03-c3e8-4450-bd9c-b3737f42624d",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed023a-3d7a-41e3-ad3f-baa309ed1ef6",
   "metadata": {},
   "source": [
    "Changes in this version\n",
    "-  Add LinearModel Class to make writing multiple layers easy\n",
    "-  Add Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "4f24c911-711a-4b69-bbe3-f52de9e9ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0) # in notebooks, this needs to be present in the cell where the random is being called\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "class Activation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        pass\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "    def backward(self, outputs):\n",
    "        pass\n",
    "    def __call__(self, arg):\n",
    "        return self.forward(arg)\n",
    "\n",
    "# ReLU function\n",
    "class Act_Linear(Activation):\n",
    "    def forward(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, outputs):\n",
    "        return 1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Act_Linear\"\n",
    "\n",
    "class Act_ReLU(Activation):\n",
    "    def forward(self, inputs):\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, outputs):\n",
    "        return np.where(outputs > 0, 1, 0)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Act_ReLU\"\n",
    "\n",
    "class Act_Tanh(Activation):\n",
    "    def forward(self, inputs):\n",
    "        return np.tanh(inputs)\n",
    "\n",
    "    def backward(self, outputs):\n",
    "        # dy/dx = 1-y**2\n",
    "        return (1 - outputs**2)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Act_Tanh\"\n",
    "\n",
    "class Act_Sigmoid(Activation):\n",
    "    def forward(self, inputs):\n",
    "        return 1/(1+np.exp(-inputs))\n",
    "\n",
    "    def backward(self, outputs):\n",
    "        # dy/dx = y*(1-y)\n",
    "        return outputs * (1-outputs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Act_Sigmoid\"\n",
    "\n",
    "class Act_Softmax(Activation):\n",
    "    def forward(self, inputs):\n",
    "        exp = np.exp(inputs)\n",
    "        return exp / np.sum(exp)\n",
    "\n",
    "    def backward(self, outputs):\n",
    "        # TODO: y_k * (1 - y_i) when i = k\n",
    "        #       y_k * (  - y_i) when i != k\n",
    "        pass\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Act_Softmax\"\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_inputs, n_neurons, activation_fn, weights=None, biases=None):\n",
    "        if activation_fn is None:\n",
    "            activation_fn = Act_Linear\n",
    "\n",
    "        if activation_fn is Act_Softmax:\n",
    "            raise Exception(\"Softmax is not supported as an activation function, use it after the output\")\n",
    "            \n",
    "        if weights is None:\n",
    "            self.weights = 0.1 * np.random.randn(n_neurons, n_inputs) # multiplying with 0.1 to keep the range within (-1, 0, 1)\n",
    "        else:\n",
    "            self.weights = weights # used to test the correction of my code\n",
    "\n",
    "        if biases is None:\n",
    "            self.biases = np.zeros((1, n_neurons))\n",
    "        else:\n",
    "            self.biases = biases\n",
    "        self.activation = activation_fn()  # new code - initialise the activation class\n",
    "        self.inputs = []\n",
    "        self.grad_act = []\n",
    "        self.grad_new = []\n",
    "        self.grad_biases = []\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # modified to execute the activation forward code\n",
    "        self.inputs = inputs\n",
    "        output_raw = np.dot(self.inputs, self.weights.T) + self.biases\n",
    "        self.output = self.activation.forward(output_raw)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, prev_grad):\n",
    "        self.grad_act = self.activation.backward(self.output) # gradient of the activation fn\n",
    "        self.grad_new = np.multiply(prev_grad, self.grad_act)\n",
    "        self.grad_weights = np.dot(self.grad_new.T, self.inputs)\n",
    "        self.grad_biases = np.sum(self.grad_new, axis=0, keepdims=True)\n",
    "        return np.dot(self.grad_new, self.weights)\n",
    "\n",
    "    def __call__(self, arg):\n",
    "        return self.forward(arg)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer(n_inp={self.weights.shape[1]},\\\n",
    "        n_neurons={self.weights.shape[0]},\\\n",
    "        activation_fn={self.activation.__repr__()})\"\n",
    "\n",
    "# Linea\n",
    "class LinearModel:\n",
    "    def __init__(self, *args):\n",
    "        self.layers = []\n",
    "        for arg in args:\n",
    "            self.layers.append(arg)\n",
    "\n",
    "    def __call__(self, arg):\n",
    "        return self.forward(arg)\n",
    "\n",
    "    def forward(self, arg):\n",
    "        out = arg\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # TODO: how to handle different dimension data for the prev gradient\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def __repr__(self):\n",
    "        head = \"LinearModel(\\n\"\n",
    "        tail = \")\"\n",
    "        body = \"\"\n",
    "        for layer in self.layers:\n",
    "            body += layer.__repr__() + \"\\n\"\n",
    "        return head + body + tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3bbfca4e-cbe0-43f8-a9e1-2cbe927c9bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5006, 0.2954, 0.0338, 0.4669]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = Layer(3, 4, Act_Tanh)\n",
    "inp = np.array([[1.,2.,3.]])\n",
    "l(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "521c0a6d-25c6-4b10-a70c-532fb6f726d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7494, 1.4989, 2.2483],\n",
       "       [0.9128, 1.8255, 2.7383],\n",
       "       [0.9989, 1.9977, 2.9966],\n",
       "       [0.782 , 1.5639, 2.3459]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.backward(np.array([[1.0]]))\n",
    "l.grad_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "ea12e9d8-2c68-4a40-9922-91bcaaeb3361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.7500e-03, -3.0817e-03,  1.2401e-05]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LinearModel(\n",
    "    Layer(3, 4, Act_ReLU),\n",
    "    Layer(4, 5, Act_Tanh),\n",
    "    Layer(5, 2, Act_Sigmoid)\n",
    ")\n",
    "\n",
    "m(np.array([[1.,2.,3.]]))\n",
    "m.backward(np.array([[-1.0, 2.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8d552-d891-46fb-8445-ef322e382e2b",
   "metadata": {},
   "source": [
    "## Testing my solution with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "70cb5c90-b737-4d52-93d2-5c1cea6eea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Your existing code for Activation, Layer, LinearModel here...\n",
    "\n",
    "# Initialize model\n",
    "m = LinearModel(\n",
    "    Layer(3, 4, Act_ReLU),\n",
    "    Layer(4, 5, Act_Tanh),\n",
    "    Layer(5, 2, Act_Sigmoid)\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "input_np = np.array([[1., 2., 3.]])\n",
    "output_custom = m(input_np)\n",
    "\n",
    "# Backward pass (gradient of loss w.r.t. output)\n",
    "grad_output = np.array([[-1.0, 2.0]])\n",
    "m.backward(grad_output)\n",
    "\n",
    "# Extract gradients from your model\n",
    "custom_grads = []\n",
    "for layer in m.layers:\n",
    "    custom_grads.append({\n",
    "        \"weights\": layer.grad_weights,\n",
    "        \"biases\": layer.grad_biases\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9b5321ce-dd99-4ff9-8cf0-77268027ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was generated by DeepSeek\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set seed for reproducibility (if needed)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Build equivalent PyTorch model\n",
    "pytorch_model = nn.Sequential(\n",
    "    nn.Linear(3, 4),    # Layer 1 (weights: 4x3, biases: 4)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 5),    # Layer 2 (weights: 5x4, biases: 5)\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(5, 2),    # Layer 3 (weights: 2x5, biases: 2)\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Copy weights/biases from your model to PyTorch\n",
    "with torch.no_grad():\n",
    "    # Layer 1\n",
    "    pytorch_model[0].weight.data = torch.from_numpy(m.layers[0].weights.astype(np.float32))\n",
    "    pytorch_model[0].bias.data = torch.from_numpy(m.layers[0].biases.squeeze(0).astype(np.float32))  # Squeeze batch dim\n",
    "    \n",
    "    # Layer 2\n",
    "    pytorch_model[2].weight.data = torch.from_numpy(m.layers[1].weights.astype(np.float32))\n",
    "    pytorch_model[2].bias.data = torch.from_numpy(m.layers[1].biases.squeeze(0).astype(np.float32))\n",
    "    \n",
    "    # Layer 3\n",
    "    pytorch_model[4].weight.data = torch.from_numpy(m.layers[2].weights.astype(np.float32))\n",
    "    pytorch_model[4].bias.data = torch.from_numpy(m.layers[2].biases.squeeze(0).astype(np.float32))\n",
    "\n",
    "# Forward pass\n",
    "input_torch = torch.from_numpy(input_np.astype(np.float32))\n",
    "output_torch = pytorch_model(input_torch)\n",
    "\n",
    "# Manually set gradient of loss w.r.t. output (same as your code)\n",
    "output_torch.backward(gradient=torch.tensor(grad_output, dtype=torch.float32))\n",
    "\n",
    "# Extract gradients from PyTorch\n",
    "pytorch_grads = []\n",
    "for i in [0, 2, 4]:  # Indices of Linear layers in Sequential\n",
    "    pytorch_grads.append({\n",
    "        \"weights\": pytorch_model[i].weight.grad.numpy(),\n",
    "        \"biases\": pytorch_model[i].bias.grad.numpy().reshape(1, -1)  # Match your (1, n_neurons) shape\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "911ed396-1150-40cf-894f-806ecb6c8248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': array([[0.0014, 0.0027, 0.0041],\n",
      "       [0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 0.    ]]), 'biases': array([[0.0014, 0.    , 0.    , 0.    ]])}, {'weights': array([[ 0.0033,  0.    ,  0.    ,  0.    ],\n",
      "       [ 0.0015,  0.    ,  0.    ,  0.    ],\n",
      "       [ 0.0078,  0.    ,  0.    ,  0.    ],\n",
      "       [-0.0009,  0.    ,  0.    ,  0.    ],\n",
      "       [ 0.0047,  0.    ,  0.    ,  0.    ]]), 'biases': array([[ 0.0316,  0.0149,  0.0756, -0.0085,  0.0454]])}, {'weights': array([[ 0.0042, -0.0019, -0.001 ,  0.0008, -0.0012],\n",
      "       [-0.0084,  0.0037,  0.0021, -0.0016,  0.0024]]), 'biases': array([[-0.25,  0.5 ]])}]\n",
      "\n",
      "-----------------------------*******===-------------------===******-------------\n",
      "\n",
      "[{'weights': array([[0.0014, 0.0027, 0.0041],\n",
      "       [0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 0.    ],\n",
      "       [0.    , 0.    , 0.    ]], dtype=float32), 'biases': array([[0.0014, 0.    , 0.    , 0.    ]], dtype=float32)}, {'weights': array([[ 0.0033,  0.    ,  0.    ,  0.    ],\n",
      "       [ 0.0015,  0.    ,  0.    ,  0.    ],\n",
      "       [ 0.0078,  0.    ,  0.    ,  0.    ],\n",
      "       [-0.0009, -0.    , -0.    , -0.    ],\n",
      "       [ 0.0047,  0.    ,  0.    ,  0.    ]], dtype=float32), 'biases': array([[ 0.0316,  0.0149,  0.0756, -0.0085,  0.0454]], dtype=float32)}, {'weights': array([[ 0.0042, -0.0019, -0.001 ,  0.0008, -0.0012],\n",
      "       [-0.0084,  0.0037,  0.0021, -0.0016,  0.0024]], dtype=float32), 'biases': array([[-0.25,  0.5 ]], dtype=float32)}]\n"
     ]
    }
   ],
   "source": [
    "print(custom_grads)\n",
    "print('\\n-----------------------------*******===-------------------===******-------------\\n')\n",
    "print(pytorch_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08f954-c408-4628-b104-5ba1202e6c74",
   "metadata": {},
   "source": [
    "**My backprop seems correct !**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
